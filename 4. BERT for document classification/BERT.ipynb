{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "G-oA8fiRfsHL",
        "outputId": "5706ff76-5f82-43b8-86da-6a1b7c76e39a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers\n",
        "\n",
        "'''\n",
        "# download Kaggle true fake news dataset\n",
        "!pip install kaggle\n",
        "import os\n",
        "os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/content/drive/MyDrive/Colab Notebooks/CS6120 NLP/Assignment 6 BERT\"\n",
        "!kaggle datasets download -d clmentbisaillon/fake-and-real-news-dataset\n",
        "'''\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import model_selection                   # K fold library\n",
        "from torch.utils.data import DataLoader                 \n",
        "from torch.optim import AdamW\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "7yeucFiphkcv"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7LdR97MfXcg",
        "outputId": "b5d922bd-8f0f-42f6-d52c-e65987859ca7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check GPU resources\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('Using GPU ', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('Using CPU')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PPbQfJRiL_t",
        "outputId": "5b09523e-3d03-492a-bae6-d4cfdc3e1ea9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Prepare tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "print(tokenizer)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model configuration Settings\n",
        "#NUM_LABELS = 1\n",
        "BATCH_SIZE = 32\n",
        "MAX_LEN = 30\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 1e-5"
      ],
      "metadata": {
        "id": "q_IfNvTqwELI"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/CS6120 NLP/Assignment 6 BERT/fake-and-real-news-dataset/\"\n",
        "df_real = pd.read_csv(path + 'True.csv')\n",
        "df_fake = pd.read_csv(path + 'Fake.csv')\n",
        "\n",
        "# Add y_true\n",
        "df_real['Category'] = 1\n",
        "df_fake['Category'] = 0\n",
        "\n",
        "# Combine true news and fake news into one single file\n",
        "df = df_real.append(df_fake)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFNeWhyZJgMl",
        "outputId": "3b9dde1d-c175-40eb-9b3e-04d9ed7217e5"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-82-21ddf76171d3>:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df_real.append(df_fake)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Diminish dataset to 25% due to training time\n",
        "dataset_75, dataset_25 = train_test_split(df, test_size=0.25, shuffle=True)\n",
        "df = dataset_25"
      ],
      "metadata": {
        "id": "_dZLGU1LLZ1F"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# convert news title to input ids using BERT tokenizer\n",
        "def process_data(df, tokenizer, max_len=30):\n",
        "    \"\"\"\n",
        "    Process the data to feed into the pretrained model\n",
        "    \"\"\"\n",
        "    tokenizer_dict = tokenizer(df.title.values.tolist(), return_tensors='pt', padding=True, truncation=True, max_length=10)\n",
        "    tokens = tokenizer_dict['input_ids']\n",
        "    attention_mask = tokenizer_dict['attention_mask']\n",
        "    y = torch.tensor(df.Category.values)\n",
        "\n",
        "    return tokens, attention_mask, y\n",
        "\n",
        "print(process_data(df, tokenizer, max_len=MAX_LEN)[:10])\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "oRALgEIcOmLn",
        "outputId": "2b20defc-9ffb-4b28-c1cf-e24effd81a99"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# convert news title to input ids using BERT tokenizer\\ndef process_data(df, tokenizer, max_len=30):\\n    \"\"\"\\n    Process the data to feed into the pretrained model\\n    \"\"\"\\n    tokenizer_dict = tokenizer(df.title.values.tolist(), return_tensors=\\'pt\\', padding=True, truncation=True, max_length=10)\\n    tokens = tokenizer_dict[\\'input_ids\\']\\n    attention_mask = tokenizer_dict[\\'attention_mask\\']\\n    y = torch.tensor(df.Category.values)\\n\\n    return tokens, attention_mask, y\\n\\nprint(process_data(df, tokenizer, max_len=MAX_LEN)[:10])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "  def __init__(self, train_kf, tokenizer, max_len):\n",
        "    self.train_kf = train_kf\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "    \n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # convert news title to input ids using BERT tokenizer\n",
        "    title = self.train_kf.iloc[index][\"title\"]\n",
        "    category = self.train_kf.iloc[index][\"Category\"]\n",
        "\n",
        "    #tokenizer_dict = tokenizer.encode_plus(title, add_special_tokens = True, return_attention_mask = True, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_len)\n",
        "    tokenizer_dict = tokenizer(title, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_len)\n",
        "    input_ids = tokenizer_dict['input_ids']\n",
        "    attention_mask = tokenizer_dict['attention_mask'] \n",
        "    y = torch.tensor(category)\n",
        "\n",
        "    #print(input_ids, attention_mask, y)\n",
        "    #print(input_ids.shape, attention_mask.shape, y.shape)\n",
        "\n",
        "    return input_ids, attention_mask, y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.train_kf)"
      ],
      "metadata": {
        "id": "FPYBN_J9QKSk"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_dataloader, model, device, lr=2e-5, warmup_steps=200):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    \n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "                optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1)\n",
        "  \n",
        "    for batch in train_dataloader:\n",
        "      input_ids, attention_mask, labels = tuple(item.to(device) for item in batch)\n",
        "      \n",
        "      # without these 3 statements, despite tensor shape [batch size, seq_len] is correctly output, \n",
        "      # at here it still in [batch size, 1, seq_len] shape, so need to reshape here\n",
        "      input_ids = input_ids.squeeze(1)\n",
        "      attention_mask = attention_mask.squeeze(1)\n",
        "      labels = labels.unsqueeze(0)\n",
        "\n",
        "      optimizer.zero_grad() \n",
        "\n",
        "      # forward pass, outputs object contains the model's predictions, as well as the loss and other optional outputs.\n",
        "      outputs = model(input_ids, attention_mask, labels=labels)  \n",
        "\n",
        "      # default loss function by huggingface BERT is cross entropy\n",
        "      # extracts the loss value from the outputs object\n",
        "      loss = outputs.loss        # tensor in format \n",
        "      print(\"loss: \", loss)\n",
        "\n",
        "      loss.backward() \n",
        "      optimizer.step() \n",
        "      scheduler.step()\n",
        "\n",
        "      # outputs.logits returns the unnormalized model output for the current batch of inputs. \n",
        "      # The logits are a vector of values that represent the model's confidence scores for each class label. \n",
        "      # In the case of a binary classification task, there will be two logits, one for each class.\n",
        "      #logits = outputs.logits\n",
        "\n",
        "      #total_acc += eval_metric(logits, labels)\n",
        "\n",
        "    #loss_per_epoch = total_loss/len(train_dataloader)\n",
        "    #acc_per_epoch = total_acc/len(train_dataloader)\n",
        "\n",
        "    # return loss_per_epoch, acc_per_epoch"
      ],
      "metadata": {
        "id": "ZPMeSHRcuzbN"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(val_dataloader, model, device, lr=2e-5, warmup_steps=200):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    total_loss, total_acc = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids, attention_mask, labels = tuple(item.to(device) for item in batch)\n",
        "\n",
        "            input_ids = input_ids.squeeze(1)\n",
        "            attention_mask = attention_mask.squeeze(1)\n",
        "            labels = labels.unsqueeze(0)\n",
        "\n",
        "            # outputs produces 2 parameters: outputs.loss, outputs.logits\n",
        "            outputs = model(input_ids, attention_mask, labels=labels) \n",
        "            total_loss += outputs.loss \n",
        "\n",
        "            # store prediction\n",
        "            # outputs.logits represents the model's confidence scores for each class label\n",
        "            # argmax(-1) means taking the class with the highest predicted probability\n",
        "            prediction = outputs.logits.argmax(-1)\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "---\n",
        "            test_loss /= len(test_loader.dataset)\n",
        "            test_losses.append(test_loss)\n",
        "\n",
        "---\n",
        "            logits = outputs[0]\n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "            \n",
        "            # Calculate the accuracy for this batch of test sentences.\n",
        "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "            \n",
        "            # Accumulate the total accuracy.\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "            # Track the number of batches\n",
        "            nb_eval_steps += 1\n",
        "\n",
        "        # Report the final accuracy for this validation run.\n",
        "        print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "        print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "        "
      ],
      "metadata": {
        "id": "PyOJEFXytRxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define learning rates to test\n",
        "learning_rates = [0.0001, 0.001, 0.01]\n",
        "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "best_lr = None\n",
        "best_avg_loss = float('inf')\n",
        "\n",
        "for lr in learning_rates:\n",
        "\n",
        "    avg_loss = 0\n",
        "\n",
        "    print(\"lr\", lr)\n",
        "\n",
        "    for train_index, valid_index in kf.split(df):    # kf.split() method returns an iterator that generates a set of train and validation indices for each fold.\n",
        "\n",
        "        # Get the training and validation data for this fold\n",
        "        train_kf = df.iloc[train_index]\n",
        "        val_kf = df.iloc[valid_index]\n",
        "\n",
        "        print(len(train_kf))  # 5 fold is 80%\n",
        "        print(len(val_kf))   # 5 fold is 20%\n",
        "\n",
        "        # Get tokens and attention mask\n",
        "        train_dataset = NewsDataset(train_kf, tokenizer, max_len=MAX_LEN)\n",
        "        val_dataset = NewsDataset(val_kf, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "        # Get dataloader\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "\n",
        "            num_epochs_no_improvement = 0\n",
        "\n",
        "            # Train the model on this fold with the current learning rate\n",
        "            loss = train(train_dataloader, model, device, lr=lr, warmup_steps=200)\n",
        "            print(\"Epoch: {}, loss: {}\".format(epoch, loss))\n",
        "\n",
        "        '''      \n",
        "        # Evaluate the model on the validation data for this fold\n",
        "        loss = evaluate_model(model, fold_valid_data, fold_valid_labels)\n",
        "        avg_loss += loss / kf.n_splits\n",
        "    \n",
        "    # Update best learning rate if this one performed better\n",
        "    if avg_loss < best_avg_loss:\n",
        "        best_lr = lr\n",
        "        best_avg_loss = avg_loss\n",
        "        \n",
        "# Train a final model on the entire training set with the best learning rate\n",
        "final_model = train_model(train_data, train_labels, learning_rate=best_lr)\n",
        "\n",
        "# Evaluate the final model on the test set\n",
        "test_loss = evaluate_model(final_model, test_data, test_labels)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2YQqdGbYsVB",
        "outputId": "63f09313-34c9-41bf-ef90-46e38dbb248b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr 0.0001\n",
            "8980\n",
            "2245\n",
            "loss:  tensor(0.7639, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.7302, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.7316, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.7564, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.7139, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.7409, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.7145, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.6882, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.7018, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.7001, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.6580, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.7091, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.6954, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.6558, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.6651, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.6980, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.6805, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.6545, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.6911, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.6253, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.6490, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.6424, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.6364, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.6572, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.6323, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.6112, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.5858, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.6009, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.5551, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.5295, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.4766, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.4837, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.5844, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.4593, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.4706, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.4585, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3471, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3478, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3594, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3685, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3137, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3279, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2836, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2502, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2455, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2905, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1981, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2760, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2453, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3323, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3367, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3371, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1791, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2687, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3080, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.6175, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1351, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2671, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1429, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1625, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1850, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2619, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3281, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3250, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3599, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1087, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1702, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3684, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2506, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3138, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2771, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1160, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1838, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3417, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1847, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1476, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2284, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2267, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3032, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2334, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1384, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1550, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1057, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0767, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0874, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1869, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.4859, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.4695, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0867, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1855, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1943, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0524, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2362, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2336, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2175, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1822, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1267, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1031, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1061, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1135, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0427, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1414, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1133, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1647, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1234, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0535, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1339, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0178, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0651, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0892, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2242, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1660, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0269, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2343, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0401, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0301, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0290, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2474, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0759, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0551, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0858, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2835, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0427, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0813, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2387, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1146, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0682, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0510, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0489, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1877, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2032, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0607, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1928, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0638, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0802, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0293, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1171, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1769, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0879, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2729, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0093, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1609, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0825, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0572, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0722, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0755, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0541, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0173, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1010, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0542, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0555, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0275, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0753, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0603, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0271, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3718, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0150, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0669, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1553, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1552, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0473, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1227, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0227, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2702, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0367, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0594, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0734, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0308, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0416, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0531, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0724, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0258, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0235, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0992, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1009, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0045, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0953, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0084, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0255, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1187, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0965, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0818, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1542, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1033, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0363, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2497, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0982, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2003, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0499, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1911, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0665, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1113, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1685, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1483, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1319, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0263, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1111, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0542, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0958, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0687, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2591, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1946, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1507, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1626, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0431, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.4402, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1787, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0513, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0918, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3602, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1991, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2296, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1828, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1774, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0579, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2984, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1930, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3336, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0975, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0030, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3691, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0706, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0757, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1600, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1572, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2062, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0105, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0855, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0369, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0983, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0531, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2059, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2332, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0334, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2760, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0258, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1179, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1207, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1424, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3793, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1141, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0176, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2003, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0069, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0824, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0325, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0458, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1985, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1551, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2628, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0089, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1074, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1591, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0162, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2218, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1350, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1601, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1734, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3349, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1532, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0261, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.4117, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2568, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2896, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2927, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1110, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0285, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0057, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0590, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2066, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1191, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0230, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0366, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0103, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2515, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1829, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2016, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2370, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1853, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0528, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.3805, grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0, loss: 0.21620009030261841\n",
            "loss:  tensor(0.0333, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1543, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0834, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1451, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1332, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0620, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1476, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0090, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0261, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0649, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1062, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0069, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1489, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0305, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0766, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0072, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0849, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0349, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0858, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0144, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0648, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0456, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1398, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0663, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0647, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0072, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0184, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0994, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1471, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0437, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0678, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1521, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0423, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0082, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2137, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2399, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1881, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1380, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0064, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0330, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0714, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0371, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0071, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0425, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.1265, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0301, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0125, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0169, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0741, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0224, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0768, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0182, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0164, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0216, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0352, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0024, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0148, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0128, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0169, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0537, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0399, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0086, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0052, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0967, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0051, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0039, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0073, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0486, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0846, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0025, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.2031, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0010, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0166, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0089, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0964, grad_fn=<NllLossBackward0>)\n",
            "loss:  tensor(0.0141, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hk-ETn9jKPh"
      },
      "source": [
        "model.to(device)\n",
        "model.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMIGLuX00Akt"
      },
      "source": [
        "text_batch = [\"I love Pixar.\",\n",
        "              \"I don't care for Pixar.\",\n",
        "              \"This is such a super duper long sentence with so many words you can barely understand it oh my gosh\"]\n",
        "encoding = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True, max_length=10)\n",
        "input_ids = encoding['input_ids']\n",
        "attention_mask = encoding['attention_mask']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPilL8_R1gcm"
      },
      "source": [
        "input_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvG8DF721hQh"
      },
      "source": [
        "attention_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGyt_-iy1nNH"
      },
      "source": [
        "from torch.nn import functional as F\n",
        "labels = torch.tensor([1,0,0])\n",
        "outputs = model(input_ids, attention_mask=attention_mask)\n",
        "# softmaxed_output = softmax_fn(outputs)\n",
        "loss = F.cross_entropy(outputs.logits, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9oiMF_N1Z0s"
      },
      "source": [
        "loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tae3CI8T7_H5"
      },
      "source": [
        "loss.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpXjYh328DnG"
      },
      "source": [
        "from transformers import AdamW\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOhYr4muBlNR"
      },
      "source": [
        "optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE2e2hKaBxN5"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNEmxLIjCkXS"
      },
      "source": [
        "!tensorboard dev upload --logdir \"/content/drive/MyDrive/Colab Notebooks/CS6120 NLP/CS6120 Project 6\" --name \"image captioning 6.5 epochs\"\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}