{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "G-oA8fiRfsHL",
        "outputId": "5706ff76-5f82-43b8-86da-6a1b7c76e39a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers\n",
        "\n",
        "'''\n",
        "# download Kaggle true fake news dataset\n",
        "!pip install kaggle\n",
        "import os\n",
        "os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/content/drive/MyDrive/Colab Notebooks/CS6120 NLP/Assignment 6 BERT\"\n",
        "!kaggle datasets download -d clmentbisaillon/fake-and-real-news-dataset\n",
        "'''\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import model_selection                   # K fold library\n",
        "from torch.utils.data import DataLoader                 \n",
        "from torch.optim import AdamW\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, average_precision_score, average_precision_score\n",
        "from sklearn.metrics import classification_report, plot_confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import make_scorer  \n",
        "from sklearn import metrics "
      ],
      "metadata": {
        "id": "7yeucFiphkcv"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7LdR97MfXcg",
        "outputId": "b5d922bd-8f0f-42f6-d52c-e65987859ca7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check GPU resources\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('Using GPU ', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('Using CPU')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PPbQfJRiL_t",
        "outputId": "5b09523e-3d03-492a-bae6-d4cfdc3e1ea9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Prepare tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "print(tokenizer)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model configuration Settings\n",
        "#NUM_LABELS = 1\n",
        "BATCH_SIZE = 32\n",
        "MAX_LEN = 30\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 1e-5"
      ],
      "metadata": {
        "id": "q_IfNvTqwELI"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/CS6120 NLP/Assignment 6 BERT/fake-and-real-news-dataset/\"\n",
        "df_real = pd.read_csv(path + 'True.csv')\n",
        "df_fake = pd.read_csv(path + 'Fake.csv')\n",
        "\n",
        "# Add y_true\n",
        "df_real['Category'] = 1\n",
        "df_fake['Category'] = 0\n",
        "\n",
        "# Combine true news and fake news into one single file\n",
        "df = df_real.append(df_fake)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFNeWhyZJgMl",
        "outputId": "3b9dde1d-c175-40eb-9b3e-04d9ed7217e5"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-82-21ddf76171d3>:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df_real.append(df_fake)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Diminish dataset to 25% due to training time\n",
        "dataset_75, dataset_25 = train_test_split(df, test_size=0.25, shuffle=True)\n",
        "df = dataset_25"
      ],
      "metadata": {
        "id": "_dZLGU1LLZ1F"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# convert news title to input ids using BERT tokenizer\n",
        "def process_data(df, tokenizer, max_len=30):\n",
        "    \"\"\"\n",
        "    Process the data to feed into the pretrained model\n",
        "    \"\"\"\n",
        "    tokenizer_dict = tokenizer(df.title.values.tolist(), return_tensors='pt', padding=True, truncation=True, max_length=10)\n",
        "    tokens = tokenizer_dict['input_ids']\n",
        "    attention_mask = tokenizer_dict['attention_mask']\n",
        "    y = torch.tensor(df.Category.values)\n",
        "\n",
        "    return tokens, attention_mask, y\n",
        "\n",
        "print(process_data(df, tokenizer, max_len=MAX_LEN)[:10])\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "oRALgEIcOmLn",
        "outputId": "2b20defc-9ffb-4b28-c1cf-e24effd81a99"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# convert news title to input ids using BERT tokenizer\\ndef process_data(df, tokenizer, max_len=30):\\n    \"\"\"\\n    Process the data to feed into the pretrained model\\n    \"\"\"\\n    tokenizer_dict = tokenizer(df.title.values.tolist(), return_tensors=\\'pt\\', padding=True, truncation=True, max_length=10)\\n    tokens = tokenizer_dict[\\'input_ids\\']\\n    attention_mask = tokenizer_dict[\\'attention_mask\\']\\n    y = torch.tensor(df.Category.values)\\n\\n    return tokens, attention_mask, y\\n\\nprint(process_data(df, tokenizer, max_len=MAX_LEN)[:10])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "  def __init__(self, train_kf, tokenizer, max_len):\n",
        "    self.train_kf = train_kf\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "    \n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # convert news title to input ids using BERT tokenizer\n",
        "    title = self.train_kf.iloc[index][\"title\"]\n",
        "    category = self.train_kf.iloc[index][\"Category\"]\n",
        "\n",
        "    #tokenizer_dict = tokenizer.encode_plus(title, add_special_tokens = True, return_attention_mask = True, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_len)\n",
        "    tokenizer_dict = tokenizer(title, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_len)\n",
        "    input_ids = tokenizer_dict['input_ids']\n",
        "    attention_mask = tokenizer_dict['attention_mask'] \n",
        "    y = torch.tensor(category)\n",
        "\n",
        "    #print(input_ids, attention_mask, y)\n",
        "    #print(input_ids.shape, attention_mask.shape, y.shape)\n",
        "\n",
        "    return input_ids, attention_mask, y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.train_kf)"
      ],
      "metadata": {
        "id": "FPYBN_J9QKSk"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_dataloader, model, device, lr=2e-5, warmup_steps=200):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    \n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "                optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1)\n",
        "    batch_loss = 0\n",
        "  \n",
        "    for batch in train_dataloader:\n",
        "      input_ids, attention_mask, labels = tuple(item.to(device) for item in batch)\n",
        "      \n",
        "      # without these 3 statements, despite tensor shape [batch size, seq_len] is correctly output, \n",
        "      # at here it still in [batch size, 1, seq_len] shape, so need to reshape here\n",
        "      input_ids = input_ids.squeeze(1)\n",
        "      attention_mask = attention_mask.squeeze(1)\n",
        "      labels = labels.unsqueeze(0)\n",
        "\n",
        "      optimizer.zero_grad() \n",
        "\n",
        "      # forward pass, outputs object contains the model's predictions, as well as the loss and other optional outputs.\n",
        "      outputs = model(input_ids, attention_mask, labels=labels)  \n",
        "\n",
        "      # default loss function by huggingface BERT - binary_cls: nll, multiclass: cross entropy\n",
        "      # extracts the loss value from the outputs object\n",
        "      batch_loss += outputs.loss        # tensor in format \n",
        "      print(\"loss: \", outputs.loss)\n",
        "\n",
        "      loss.backward() \n",
        "      optimizer.step() \n",
        "      scheduler.step()\n",
        "    \n",
        "    return batch_loss/len(train_dataloader)"
      ],
      "metadata": {
        "id": "ZPMeSHRcuzbN"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(val_dataloader, model, device, lr=2e-5, warmup_steps=200):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    total_loss, total_acc = 0, 0\n",
        "    y_val = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids, attention_mask, labels = tuple(item.to(device) for item in batch)\n",
        "\n",
        "            input_ids = input_ids.squeeze(1)\n",
        "            attention_mask = attention_mask.squeeze(1)\n",
        "            labels = labels.unsqueeze(0)\n",
        "\n",
        "            # outputs produces 2 parameters: outputs.loss, outputs.logits\n",
        "            outputs = model(input_ids, attention_mask, labels=labels) \n",
        "            #val_loss += outputs.loss \n",
        "\n",
        "            # store prediction\n",
        "            # outputs.logits represents the model's confidence scores for each class label\n",
        "            # argmax(-1) means taking the class with the highest predicted probability\n",
        "            y_val.append(outputs.logits.argmax(-1))\n",
        "               \n",
        "    print(classification_report(y_val, labels.to('cpu').numpy()))  \n",
        "        "
      ],
      "metadata": {
        "id": "PyOJEFXytRxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define learning rates to test\n",
        "learning_rates = [0.0001, 0.001, 0.01]\n",
        "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "best_lr = None\n",
        "best_avg_loss = float('inf')\n",
        "\n",
        "for lr in learning_rates:\n",
        "\n",
        "    avg_loss = 0\n",
        "\n",
        "    print(\"lr\", lr)\n",
        "\n",
        "    for train_index, valid_index in kf.split(df):    # kf.split() method returns an iterator that generates a set of train and validation indices for each fold.\n",
        "\n",
        "        # Get the training and validation data for this fold\n",
        "        train_kf = df.iloc[train_index]\n",
        "        val_kf = df.iloc[valid_index]\n",
        "\n",
        "        print(len(train_kf))  # 5 fold is 80%\n",
        "        print(len(val_kf))   # 5 fold is 20%\n",
        "\n",
        "        # Get tokens and attention mask\n",
        "        train_dataset = NewsDataset(train_kf, tokenizer, max_len=MAX_LEN)\n",
        "        val_dataset = NewsDataset(val_kf, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "        # Get dataloader\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "        previous_loss = 100\n",
        "        early_stopping_patience = 4\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "\n",
        "            num_epochs_no_improvement = 0\n",
        "\n",
        "            # Train the model on this fold with the current learning rate\n",
        "            loss = train(train_dataloader, model, device, lr=lr, warmup_steps=200)\n",
        "            print(\"Epoch: {}, loss: {}\".format(epoch, loss))\n",
        "\n",
        "            if loss < previous_loss:\n",
        "                previous_loss = loss\n",
        "                num_epochs_no_improvement = 0\n",
        "            else:\n",
        "                num_epochs_no_improvement += 1\n",
        "                if num_epochs_no_improvement == early_stopping_patience:\n",
        "                    print(f\"Validation loss has not improved for {early_stopping_patience} epochs. Stopping training.\")\n",
        "                    return"
      ],
      "metadata": {
        "id": "M2YQqdGbYsVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hk-ETn9jKPh"
      },
      "source": [
        "model.to(device)\n",
        "model.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMIGLuX00Akt"
      },
      "source": [
        "text_batch = [\"I love Pixar.\",\n",
        "              \"I don't care for Pixar.\",\n",
        "              \"This is such a super duper long sentence with so many words you can barely understand it oh my gosh\"]\n",
        "encoding = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True, max_length=10)\n",
        "input_ids = encoding['input_ids']\n",
        "attention_mask = encoding['attention_mask']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPilL8_R1gcm"
      },
      "source": [
        "input_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvG8DF721hQh"
      },
      "source": [
        "attention_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGyt_-iy1nNH"
      },
      "source": [
        "from torch.nn import functional as F\n",
        "labels = torch.tensor([1,0,0])\n",
        "outputs = model(input_ids, attention_mask=attention_mask)\n",
        "# softmaxed_output = softmax_fn(outputs)\n",
        "loss = F.cross_entropy(outputs.logits, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9oiMF_N1Z0s"
      },
      "source": [
        "loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tae3CI8T7_H5"
      },
      "source": [
        "loss.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpXjYh328DnG"
      },
      "source": [
        "from transformers import AdamW\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOhYr4muBlNR"
      },
      "source": [
        "optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE2e2hKaBxN5"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNEmxLIjCkXS"
      },
      "source": [
        "!tensorboard dev upload --logdir \"/content/drive/MyDrive/Colab Notebooks/CS6120 NLP/CS6120 Project 6\" --name \"image captioning 6.5 epochs\"\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}