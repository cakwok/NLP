{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CS6120 NLP Assignment 7 - BERT to perform text classification<br>\n",
        "Wing Man Casca, Kwok<br>\n",
        "Apr 4 2023"
      ],
      "metadata": {
        "id": "ctA8ESBByD9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "G-oA8fiRfsHL",
        "outputId": "435691b9-dbb6-4c0e-a451-01c487dc7147",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "Sv89mcGSyvvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "'''\n",
        "# download Kaggle true fake news dataset\n",
        "!pip install kaggle\n",
        "import os\n",
        "os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/content/drive/MyDrive/Colab Notebooks\"\n",
        "!kaggle datasets download -d clmentbisaillon/fake-and-real-news-dataset\n",
        "'''\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import model_selection                   # K fold library\n",
        "from torch.utils.data import DataLoader                 \n",
        "from torch.optim import AdamW\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, average_precision_score, average_precision_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import make_scorer  \n",
        "from sklearn import metrics \n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yeucFiphkcv",
        "outputId": "b2489613-48a3-4f80-ea4b-0dc0f1e354d3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7LdR97MfXcg",
        "outputId": "bf8bc189-3e63-4eac-cb28-3ed86f42acd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check GPU resources\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('Using GPU ', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('Using CPU')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU  Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PPbQfJRiL_t",
        "outputId": "0d8afb19-8693-47e8-ee09-82bd1c35ea80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Prepare tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "print(tokenizer)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model configuration Settings\n",
        "#NUM_LABELS = 1\n",
        "BATCH_SIZE = 32\n",
        "MAX_LEN = 32\n",
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "q_IfNvTqwELI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/CS6120_NLP/BERT/fake-and-real-news-dataset/\"\n",
        "df_real = pd.read_csv(path + 'True.csv')\n",
        "df_fake = pd.read_csv(path + 'Fake.csv')\n",
        "\n",
        "# Add y_true\n",
        "df_real['Category'] = 1\n",
        "df_fake['Category'] = 0\n",
        "\n",
        "# Combine true news and fake news into one single file\n",
        "df = df_real.append(df_fake)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFNeWhyZJgMl",
        "outputId": "30980194-3b25-4356-aa30-8cbfb5ce6c16"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-77d98d8728ce>:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df_real.append(df_fake)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Diminish dataset to 1% due to training time\n",
        "dataset_99, dataset_1 = train_test_split(df, test_size=0.01, shuffle=True)\n",
        "df = dataset_1"
      ],
      "metadata": {
        "id": "_dZLGU1LLZ1F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset for dataloading\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "  def __init__(self, train_kf, tokenizer, max_len):\n",
        "    self.train_kf = train_kf\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "    \n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # convert news docment to input ids using BERT tokenizer\n",
        "    text = self.train_kf.iloc[index][\"text\"]\n",
        "    category = self.train_kf.iloc[index][\"Category\"]\n",
        "\n",
        "    tokenizer_dict = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_len)\n",
        "    input_ids = tokenizer_dict['input_ids']\n",
        "    attention_mask = tokenizer_dict['attention_mask'] \n",
        "    y = torch.tensor(category)\n",
        "\n",
        "    return input_ids, attention_mask, y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.train_kf)"
      ],
      "metadata": {
        "id": "FPYBN_J9QKSk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model training \n",
        "def train(train_dataloader, model, device, lr=2e-5, warmup_steps=200):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    \n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "                optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1)\n",
        "    batch_loss = 0\n",
        "  \n",
        "    for batch in train_dataloader:\n",
        "      input_ids, attention_mask, labels = tuple(item.to(device) for item in batch)\n",
        "      \n",
        "      # without these 3 statements, despite tensor shape [batch size, seq_len] is correctly output at the dataloading step, \n",
        "      # at here it has become [batch size, 1, seq_len] shape, so need to reshape here\n",
        "      input_ids = input_ids.squeeze(1)\n",
        "      attention_mask = attention_mask.squeeze(1)\n",
        "      labels = labels.unsqueeze(0)\n",
        "\n",
        "      optimizer.zero_grad() \n",
        "\n",
        "      # forward pass, outputs object contains the model's predictions, as well as the loss and other optional outputs.\n",
        "      outputs = model(input_ids, attention_mask, labels=labels)  \n",
        "\n",
        "      # default loss function by huggingface BERT - binary_cls: nll, multiclass: cross entropy\n",
        "      # extracts the loss value from the outputs object\n",
        "      loss = outputs.loss\n",
        "      batch_loss += outputs.loss        # tensor in format \n",
        "\n",
        "      loss.backward() \n",
        "      optimizer.step() \n",
        "      scheduler.step()\n",
        "    \n",
        "    return batch_loss/len(train_dataloader)"
      ],
      "metadata": {
        "id": "ZPMeSHRcuzbN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validate model\n",
        "def validate(val_dataloader, model, device, whole_ds=False):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    y_val = []\n",
        "    labels_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids, attention_mask, labels = tuple(item.to(device) for item in batch)\n",
        "\n",
        "            input_ids = input_ids.squeeze(1)\n",
        "            attention_mask = attention_mask.squeeze(1)\n",
        "            labels = labels.unsqueeze(0)\n",
        "\n",
        "            # outputs produces 2 parameters: outputs.loss, outputs.logits\n",
        "            outputs = model(input_ids, attention_mask, labels=labels) \n",
        "\n",
        "            # store prediction\n",
        "            # outputs.logits represents the model's confidence scores for each class label\n",
        "            y_val.append(outputs.logits.argmax(axis=1).tolist())\n",
        "            labels_list.append(labels.tolist())\n",
        "\n",
        "            #print(\"outputs.logits.argmax(axis=1)\\n\", outputs.logits.argmax(axis=1))\n",
        "\n",
        "            y_val_flatten = [item for sublist in y_val for item in sublist]\n",
        "            labels_flatten = [item for sublist1 in labels_list for sublist2 in sublist1 for item in sublist2]\n",
        "\n",
        "    print(\"y_val_flatten\\n\", y_val_flatten)  \n",
        "    print(\"labels_flatten\\n\", labels_flatten)\n",
        "    print(\"F1 score\\n\", f1_score(y_val_flatten, labels_flatten, average='micro'))\n",
        "    if whole_ds == True:\n",
        "        print(classification_report(y_val_flatten, labels_flatten))\n",
        "        "
      ],
      "metadata": {
        "id": "PyOJEFXytRxe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define learning rates to test\n",
        "learning_rates = [0.0001, 0.001, 0.01]\n",
        "\n",
        "# Define k fold\n",
        "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "early_stopping_patience = 4\n",
        "\n",
        "# Perform KFold cross validation to finetune the language model\n",
        "# train each fold with 3 different lr and select the best\n",
        "# kf.split() method returns an iterator that generates a set of train and validation indices for each fold.\n",
        "\n",
        "for fold, (train_index, valid_index) in enumerate(kf.split(df)):    \n",
        "\n",
        "    # Get the training and validation data for this fold\n",
        "    train_kf = df.iloc[train_index]\n",
        "    val_kf = df.iloc[valid_index]\n",
        "\n",
        "    print(\"fold\", fold)\n",
        "    print(len(train_kf))  # 5 fold is 80%\n",
        "    print(len(val_kf))   # 5 fold is 20%\n",
        "\n",
        "    # Get tokens and attention mask\n",
        "    train_dataset = NewsDataset(train_kf, tokenizer, max_len=MAX_LEN)\n",
        "    val_dataset = NewsDataset(val_kf, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "    # Get dataloader\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        print(\"lr\", lr)\n",
        "        best_avg_loss = float('inf')\n",
        "\n",
        "        # reinitialize model weight after individual training\n",
        "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "\n",
        "            # Train the model on this fold with the current learning rate\n",
        "            loss = train(train_dataloader, model, device, lr=lr, warmup_steps=200)\n",
        "            print(\"Epoch: {}, loss: {}\".format(epoch, loss))\n",
        "\n",
        "            # determine if we need early stop\n",
        "            if loss < best_avg_loss:\n",
        "                best_avg_loss = loss\n",
        "                num_epochs_no_improvement = 0\n",
        "            else:\n",
        "                num_epochs_no_improvement += 1\n",
        "                if num_epochs_no_improvement == early_stopping_patience:\n",
        "                    print(f\"Validation loss has not improved for {early_stopping_patience} epochs. Stopping training.\")\n",
        "                    break\n",
        "\n",
        "        # check F1 score of validation set after training\n",
        "        validate(val_dataloader, model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2YQqdGbYsVB",
        "outputId": "19943b82-78b3-4e3f-8119-1bd012ffd811"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 0\n",
            "359\n",
            "90\n",
            "lr 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 0.6815046072006226\n",
            "Epoch: 1, loss: 0.5478965044021606\n",
            "Epoch: 2, loss: 0.3821585178375244\n",
            "Epoch: 3, loss: 0.2740594446659088\n",
            "Epoch: 4, loss: 0.21389099955558777\n",
            "Epoch: 5, loss: 0.16713014245033264\n",
            "Epoch: 6, loss: 0.1334145963191986\n",
            "Epoch: 7, loss: 0.1109265685081482\n",
            "Epoch: 8, loss: 0.09352567046880722\n",
            "Epoch: 9, loss: 0.07697620242834091\n",
            "Epoch: 10, loss: 0.06284703314304352\n",
            "Epoch: 11, loss: 0.052361488342285156\n",
            "Epoch: 12, loss: 0.04226803779602051\n",
            "Epoch: 13, loss: 0.03532828763127327\n",
            "Epoch: 14, loss: 0.027576133608818054\n",
            "Epoch: 15, loss: 0.022041842341423035\n",
            "Epoch: 16, loss: 0.017797401174902916\n",
            "Epoch: 17, loss: 0.014349168166518211\n",
            "Epoch: 18, loss: 0.011440555565059185\n",
            "Epoch: 19, loss: 0.009380526840686798\n",
            "y_val_flatten\n",
            " [1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0]\n",
            "labels_flatten\n",
            " [1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0]\n",
            "F1 score\n",
            " 1.0\n",
            "lr 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 0.5237127542495728\n",
            "Epoch: 1, loss: 0.15846599638462067\n",
            "Epoch: 2, loss: 0.04347994551062584\n",
            "Epoch: 3, loss: 0.015885667875409126\n",
            "Epoch: 4, loss: 0.007186021655797958\n",
            "Epoch: 5, loss: 0.0012055672705173492\n",
            "Epoch: 6, loss: 9.608168329577893e-05\n",
            "Epoch: 7, loss: 1.7128557374235243e-05\n",
            "Epoch: 8, loss: 4.056032139487797e-06\n",
            "Epoch: 9, loss: 1.1642414392554201e-06\n",
            "Epoch: 10, loss: 4.665482435939339e-07\n",
            "Epoch: 11, loss: 2.4569175138822175e-07\n",
            "Epoch: 12, loss: 1.4626198208134156e-07\n",
            "Epoch: 13, loss: 1.1047259107499485e-07\n",
            "Epoch: 14, loss: 5.654457879700203e-08\n",
            "Epoch: 15, loss: 1.459071974352355e-08\n",
            "Epoch: 16, loss: 2.1730859334212482e-09\n",
            "Epoch: 17, loss: 1.2417633588057697e-09\n",
            "Epoch: 18, loss: 3.1044083970144243e-10\n",
            "Epoch: 19, loss: 3.1044083970144243e-10\n",
            "y_val_flatten\n",
            " [1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0]\n",
            "labels_flatten\n",
            " [1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0]\n",
            "F1 score\n",
            " 1.0\n",
            "lr 0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 0.37498709559440613\n",
            "Epoch: 1, loss: 0.12406545877456665\n",
            "Epoch: 2, loss: 0.46116065979003906\n",
            "Epoch: 3, loss: 0.06247485801577568\n",
            "Epoch: 4, loss: 0.2923194169998169\n",
            "Epoch: 5, loss: 0.29637616872787476\n",
            "Epoch: 6, loss: 0.03540700301527977\n",
            "Epoch: 7, loss: 0.39987850189208984\n",
            "Epoch: 8, loss: 0.2582245469093323\n",
            "Epoch: 9, loss: 1.9356141090393066\n",
            "Epoch: 10, loss: 0.9297534227371216\n",
            "Validation loss has not improved for 4 epochs. Stopping training.\n",
            "y_val_flatten\n",
            " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "labels_flatten\n",
            " [1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0]\n",
            "F1 score\n",
            " 0.5333333333333333\n",
            "fold 1\n",
            "359\n",
            "90\n",
            "lr 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 0.6801482439041138\n",
            "Epoch: 1, loss: 0.5914452075958252\n",
            "Epoch: 2, loss: 0.5346781015396118\n",
            "Epoch: 3, loss: 0.46095508337020874\n",
            "Epoch: 4, loss: 0.40346965193748474\n",
            "Epoch: 5, loss: 0.35374388098716736\n",
            "Epoch: 6, loss: 0.30199962854385376\n",
            "Epoch: 7, loss: 0.2624644935131073\n",
            "Epoch: 8, loss: 0.22376707196235657\n",
            "Epoch: 9, loss: 0.18876442313194275\n",
            "Epoch: 10, loss: 0.15299168229103088\n",
            "Epoch: 11, loss: 0.12951891124248505\n",
            "Epoch: 12, loss: 0.10710626095533371\n",
            "Epoch: 13, loss: 0.0870715007185936\n",
            "Epoch: 14, loss: 0.06957133114337921\n",
            "Epoch: 15, loss: 0.0551266223192215\n",
            "Epoch: 16, loss: 0.04402410238981247\n",
            "Epoch: 17, loss: 0.0374133326113224\n",
            "Epoch: 18, loss: 0.0296679325401783\n",
            "Epoch: 19, loss: 0.02625170163810253\n",
            "y_val_flatten\n",
            " [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
            "labels_flatten\n",
            " [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
            "F1 score\n",
            " 1.0\n",
            "lr 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 0.5624526739120483\n",
            "Epoch: 1, loss: 0.20680934190750122\n",
            "Epoch: 2, loss: 0.056972842663526535\n",
            "Epoch: 3, loss: 0.014807017520070076\n",
            "Epoch: 4, loss: 0.0013200503308326006\n",
            "Epoch: 5, loss: 0.00018010876374319196\n",
            "Epoch: 6, loss: 3.281419049017131e-05\n",
            "Epoch: 7, loss: 5.758613951911684e-06\n",
            "Epoch: 8, loss: 1.4407992239284795e-06\n",
            "Epoch: 9, loss: 5.408322749644867e-07\n",
            "Epoch: 10, loss: 2.5150143301289063e-07\n",
            "Epoch: 11, loss: 1.498098782803936e-07\n",
            "Epoch: 12, loss: 1.1486311279895745e-07\n",
            "Epoch: 13, loss: 5.747590137161751e-08\n",
            "Epoch: 14, loss: 1.5078555293257523e-08\n",
            "Epoch: 15, loss: 1.8626450382086546e-09\n",
            "Epoch: 16, loss: 9.313225191043273e-10\n",
            "Epoch: 17, loss: 3.1487568108445885e-09\n",
            "Epoch: 18, loss: 0.0\n",
            "Epoch: 19, loss: 0.0\n",
            "y_val_flatten\n",
            " [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
            "labels_flatten\n",
            " [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
            "F1 score\n",
            " 1.0\n",
            "lr 0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 0.295303612947464\n",
            "Epoch: 1, loss: 0.078341044485569\n",
            "Epoch: 2, loss: 0.07774752378463745\n",
            "Epoch: 3, loss: 0.2685577869415283\n",
            "Epoch: 4, loss: 0.28067412972450256\n",
            "Epoch: 5, loss: 0.1634032130241394\n",
            "Epoch: 6, loss: 0.02445177733898163\n",
            "Epoch: 7, loss: 0.05546518787741661\n",
            "Epoch: 8, loss: 3.7344987504184246e-05\n",
            "Epoch: 9, loss: 1.4811649322509766\n",
            "Epoch: 10, loss: 1.4612674713134766\n",
            "Epoch: 11, loss: 0.7027384638786316\n",
            "Epoch: 12, loss: 0.8168746829032898\n",
            "Validation loss has not improved for 4 epochs. Stopping training.\n",
            "y_val_flatten\n",
            " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "labels_flatten\n",
            " [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
            "F1 score\n",
            " 0.4888888888888889\n",
            "fold 2\n",
            "359\n",
            "90\n",
            "lr 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 0.6738466024398804\n",
            "Epoch: 1, loss: 0.5766671895980835\n",
            "Epoch: 2, loss: 0.4730331599712372\n",
            "Epoch: 3, loss: 0.39425286650657654\n",
            "Epoch: 4, loss: 0.3371387720108032\n",
            "Epoch: 5, loss: 0.2852504253387451\n",
            "Epoch: 6, loss: 0.2459568977355957\n",
            "Epoch: 7, loss: 0.20998826622962952\n",
            "Epoch: 8, loss: 0.17586080729961395\n",
            "Epoch: 9, loss: 0.14801761507987976\n",
            "Epoch: 10, loss: 0.12129811942577362\n",
            "Epoch: 11, loss: 0.10454405844211578\n",
            "Epoch: 12, loss: 0.08438308537006378\n",
            "Epoch: 13, loss: 0.06891581416130066\n",
            "Epoch: 14, loss: 0.05767641216516495\n",
            "Epoch: 15, loss: 0.047928422689437866\n",
            "Epoch: 16, loss: 0.041324831545352936\n",
            "Epoch: 17, loss: 0.03456743806600571\n",
            "Epoch: 18, loss: 0.025738000869750977\n",
            "Epoch: 19, loss: 0.020817792043089867\n",
            "y_val_flatten\n",
            " [1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1]\n",
            "labels_flatten\n",
            " [1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1]\n",
            "F1 score\n",
            " 1.0\n",
            "lr 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 0.5455480813980103\n",
            "Epoch: 1, loss: 0.19675123691558838\n",
            "Epoch: 2, loss: 0.059578027576208115\n",
            "Epoch: 3, loss: 0.023638024926185608\n",
            "Epoch: 4, loss: 0.002290291478857398\n",
            "Epoch: 5, loss: 0.00040708843152970076\n",
            "Epoch: 6, loss: 7.156375068007037e-05\n",
            "Epoch: 7, loss: 1.5693822206230834e-05\n",
            "Epoch: 8, loss: 3.46890351465845e-06\n",
            "Epoch: 9, loss: 1.1028186008843477e-06\n",
            "Epoch: 10, loss: 4.752849349642929e-07\n",
            "Epoch: 11, loss: 2.5841984552243957e-07\n",
            "Epoch: 12, loss: 1.4404454873329087e-07\n",
            "Epoch: 13, loss: 1.1393179022434197e-07\n",
            "Epoch: 14, loss: 5.995943297421036e-08\n",
            "Epoch: 15, loss: 1.5078555293257523e-08\n",
            "Epoch: 16, loss: 3.068004161832505e-07\n",
            "Epoch: 17, loss: 6.208816794028849e-10\n",
            "Epoch: 18, loss: 0.0\n",
            "Epoch: 19, loss: 0.0\n",
            "y_val_flatten\n",
            " [1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1]\n",
            "labels_flatten\n",
            " [1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1]\n",
            "F1 score\n",
            " 1.0\n",
            "lr 0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 0.44918638467788696\n",
            "Epoch: 1, loss: 0.36764252185821533\n",
            "Epoch: 2, loss: 0.009587215259671211\n",
            "Epoch: 3, loss: 0.43280646204948425\n",
            "Epoch: 4, loss: 0.8796525001525879\n",
            "Epoch: 5, loss: 1.1880242824554443\n",
            "Epoch: 6, loss: 0.747535228729248\n",
            "Validation loss has not improved for 4 epochs. Stopping training.\n",
            "y_val_flatten\n",
            " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "labels_flatten\n",
            " [1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1]\n",
            "F1 score\n",
            " 0.4444444444444444\n",
            "fold 3\n",
            "359\n",
            "90\n",
            "lr 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 0.6944572925567627\n",
            "Epoch: 1, loss: 0.6286720633506775\n",
            "Epoch: 2, loss: 0.5370960235595703\n",
            "Epoch: 3, loss: 0.44843846559524536\n",
            "Epoch: 4, loss: 0.3628273904323578\n",
            "Epoch: 5, loss: 0.3050786852836609\n",
            "Epoch: 6, loss: 0.25128424167633057\n",
            "Epoch: 7, loss: 0.1996871680021286\n",
            "Epoch: 8, loss: 0.15627875924110413\n",
            "Epoch: 9, loss: 0.12514711916446686\n",
            "Epoch: 10, loss: 0.0965019017457962\n",
            "Epoch: 11, loss: 0.07332255691289902\n",
            "Epoch: 12, loss: 0.05900314450263977\n",
            "Epoch: 13, loss: 0.045581601560115814\n",
            "Epoch: 14, loss: 0.03285162150859833\n",
            "Epoch: 15, loss: 0.02686314284801483\n",
            "Epoch: 16, loss: 0.019699014723300934\n",
            "Epoch: 17, loss: 0.014545018784701824\n",
            "Epoch: 18, loss: 0.01171054970473051\n",
            "Epoch: 19, loss: 0.009038178250193596\n",
            "y_val_flatten\n",
            " [1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0]\n",
            "labels_flatten\n",
            " [1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0]\n",
            "F1 score\n",
            " 1.0\n",
            "lr 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 0.5343654751777649\n",
            "Epoch: 1, loss: 0.15462151169776917\n",
            "Epoch: 2, loss: 0.047387223690748215\n",
            "Epoch: 3, loss: 0.020261185243725777\n",
            "Epoch: 4, loss: 0.002940199803560972\n",
            "Epoch: 5, loss: 0.0002516770618967712\n",
            "Epoch: 6, loss: 2.916858284152113e-05\n",
            "Epoch: 7, loss: 5.198802682571113e-06\n",
            "Epoch: 8, loss: 1.299016730627045e-06\n",
            "Epoch: 9, loss: 4.4468433202382585e-07\n",
            "Epoch: 10, loss: 2.2511396480240364e-07\n",
            "Epoch: 11, loss: 1.3131648302078247e-07\n",
            "Epoch: 12, loss: 9.814365142801762e-08\n",
            "Epoch: 13, loss: 3.4458935260772705e-08\n",
            "Epoch: 14, loss: 7.140139146599722e-09\n",
            "Epoch: 15, loss: 2.1730859334212482e-09\n",
            "Epoch: 16, loss: 9.313225191043273e-10\n",
            "Epoch: 17, loss: 3.1044083970144243e-10\n",
            "Epoch: 18, loss: 0.0\n",
            "Epoch: 19, loss: 3.1044083970144243e-10\n",
            "y_val_flatten\n",
            " [1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0]\n",
            "labels_flatten\n",
            " [1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0]\n",
            "F1 score\n",
            " 1.0\n",
            "lr 0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 0.4150233864784241\n",
            "Epoch: 1, loss: 0.2253381907939911\n",
            "Epoch: 2, loss: 0.2125651091337204\n",
            "Epoch: 3, loss: 0.08583762496709824\n",
            "Epoch: 4, loss: 0.41449999809265137\n",
            "Epoch: 5, loss: 0.14732831716537476\n",
            "Epoch: 6, loss: 0.05679789185523987\n",
            "Epoch: 7, loss: 0.04826667904853821\n",
            "Epoch: 8, loss: 0.2511391341686249\n",
            "Epoch: 9, loss: 0.8311008810997009\n",
            "Epoch: 10, loss: 0.8887655138969421\n",
            "Epoch: 11, loss: 0.7157316207885742\n",
            "Validation loss has not improved for 4 epochs. Stopping training.\n",
            "y_val_flatten\n",
            " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "labels_flatten\n",
            " [1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0]\n",
            "F1 score\n",
            " 0.43333333333333335\n",
            "fold 4\n",
            "360\n",
            "89\n",
            "lr 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 0.6691948175430298\n",
            "Epoch: 1, loss: 0.5739316940307617\n",
            "Epoch: 2, loss: 0.45790597796440125\n",
            "Epoch: 3, loss: 0.383735716342926\n",
            "Epoch: 4, loss: 0.31793493032455444\n",
            "Epoch: 5, loss: 0.26698219776153564\n",
            "Epoch: 6, loss: 0.22489634156227112\n",
            "Epoch: 7, loss: 0.18529759347438812\n",
            "Epoch: 8, loss: 0.1521136462688446\n",
            "Epoch: 9, loss: 0.12010088562965393\n",
            "Epoch: 10, loss: 0.09649302065372467\n",
            "Epoch: 11, loss: 0.07742787152528763\n",
            "Epoch: 12, loss: 0.06194230541586876\n",
            "Epoch: 13, loss: 0.04915152117609978\n",
            "Epoch: 14, loss: 0.03914428874850273\n",
            "Epoch: 15, loss: 0.030947215855121613\n",
            "Epoch: 16, loss: 0.02426809072494507\n",
            "Epoch: 17, loss: 0.020137740299105644\n",
            "Epoch: 18, loss: 0.016130130738019943\n",
            "Epoch: 19, loss: 0.012820957228541374\n",
            "y_val_flatten\n",
            " [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0]\n",
            "labels_flatten\n",
            " [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0]\n",
            "F1 score\n",
            " 1.0\n",
            "lr 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 0.5427283048629761\n",
            "Epoch: 1, loss: 0.1266109049320221\n",
            "Epoch: 2, loss: 0.019600633531808853\n",
            "Epoch: 3, loss: 0.0022214194759726524\n",
            "Epoch: 4, loss: 0.00023544064606539905\n",
            "Epoch: 5, loss: 3.213989475625567e-05\n",
            "Epoch: 6, loss: 6.38822530163452e-06\n",
            "Epoch: 7, loss: 1.2920543213112978e-06\n",
            "Epoch: 8, loss: 4.547958383227524e-07\n",
            "Epoch: 9, loss: 2.297262255979149e-07\n",
            "Epoch: 10, loss: 1.3069559656742058e-07\n",
            "Epoch: 11, loss: 9.840974257713242e-08\n",
            "Epoch: 12, loss: 4.31512745535656e-08\n",
            "Epoch: 13, loss: 9.623666308300471e-09\n",
            "Epoch: 14, loss: 1.552204142996061e-09\n",
            "Epoch: 15, loss: 3.1044083970144243e-10\n",
            "Epoch: 16, loss: 3.1044083970144243e-10\n",
            "Epoch: 17, loss: 0.0\n",
            "Epoch: 18, loss: 0.0\n",
            "Epoch: 19, loss: 0.0\n",
            "y_val_flatten\n",
            " [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0]\n",
            "labels_flatten\n",
            " [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0]\n",
            "F1 score\n",
            " 1.0\n",
            "lr 0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 0.6611869931221008\n",
            "Epoch: 1, loss: 0.49268388748168945\n",
            "Epoch: 2, loss: 0.0766557902097702\n",
            "Epoch: 3, loss: 0.34869584441185\n",
            "Epoch: 4, loss: 0.9363691210746765\n",
            "Epoch: 5, loss: 0.16223829984664917\n",
            "Epoch: 6, loss: 0.1727580726146698\n",
            "Validation loss has not improved for 4 epochs. Stopping training.\n",
            "y_val_flatten\n",
            " [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0]\n",
            "labels_flatten\n",
            " [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0]\n",
            "F1 score\n",
            " 0.9887640449438202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As from the result above, 0.001 was selected as the learning rate, because it returns both the highest F1 score and least average cost."
      ],
      "metadata": {
        "id": "Dimbdp0d2QVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the whole model using the found optimal learning rate\n",
        "train_df, val_df = train_test_split(df, test_size=0.25, shuffle=True)\n",
        "\n",
        "train_dataset = NewsDataset(train_df, tokenizer, max_len=MAX_LEN)\n",
        "val_dataset = NewsDataset(val_df, tokenizer, max_len=MAX_LEN)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "lr = 0.001\n",
        "previous_loss = 100\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    loss = train(train_dataloader, model, device, lr=lr, warmup_steps=200)\n",
        "    print(\"Epoch: {}, loss: {}\".format(epoch, loss))\n",
        "\n",
        "    if loss < previous_loss:\n",
        "        previous_loss = loss\n",
        "        num_epochs_no_improvement = 0\n",
        "    else:\n",
        "        num_epochs_no_improvement += 1\n",
        "        if num_epochs_no_improvement == early_stopping_patience:\n",
        "            print(f\"Validation loss has not improved for {early_stopping_patience} epochs. Stopping training.\")\n",
        "            break\n",
        "\n",
        "validate(val_dataloader, model, device, whole_ds=True)"
      ],
      "metadata": {
        "id": "rPMph9shyVaj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31e13180-1efa-4b38-8be3-f4481ff087c3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 0.577066957950592\n",
            "Epoch: 1, loss: 0.2207532823085785\n",
            "Epoch: 2, loss: 0.054890044033527374\n",
            "Epoch: 3, loss: 0.017414940521121025\n",
            "Epoch: 4, loss: 0.0034328140318393707\n",
            "Epoch: 5, loss: 0.0006608632393181324\n",
            "Epoch: 6, loss: 0.00014309595280792564\n",
            "Epoch: 7, loss: 2.6409838028484955e-05\n",
            "Epoch: 8, loss: 6.890405984449899e-06\n",
            "Epoch: 9, loss: 2.2097719920566306e-06\n",
            "Epoch: 10, loss: 9.238717666448792e-07\n",
            "Epoch: 11, loss: 4.565173696846614e-07\n",
            "Epoch: 12, loss: 2.7533280899660895e-07\n",
            "Epoch: 13, loss: 1.6729939034121344e-07\n",
            "Epoch: 14, loss: 1.283531787521497e-07\n",
            "Epoch: 15, loss: 9.821219038030904e-08\n",
            "Epoch: 16, loss: 4.9783420053017835e-08\n",
            "Epoch: 17, loss: 1.9303776355172886e-08\n",
            "Epoch: 18, loss: 4.063952729893572e-09\n",
            "Epoch: 19, loss: 1.6933137114705232e-09\n",
            "y_val_flatten\n",
            " [1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1]\n",
            "labels_flatten\n",
            " [1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1]\n",
            "F1 score\n",
            " 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        50\n",
            "           1       1.00      1.00      1.00        63\n",
            "\n",
            "    accuracy                           1.00       113\n",
            "   macro avg       1.00      1.00      1.00       113\n",
            "weighted avg       1.00      1.00      1.00       113\n",
            "\n"
          ]
        }
      ]
    }
  ]
}